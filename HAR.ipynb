{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b95cc5d-d00c-4974-a38c-ab0eb17ec5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New class distribution: Counter({1: 557, 8: 557, 3: 557, 2: 557, 4: 557, 7: 554, 10: 554, 0: 552, 5: 551, 9: 547, 6: 544})\n",
      "\n",
      "Performing K-Fold cross-validation for Random Forest...\n",
      "Best Parameters for Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Cross-Validation F1 Macro Score for Random Forest: 0.9550925416266495\n",
      "\n",
      "Performing K-Fold cross-validation for XGBoost...\n",
      "Best Parameters for XGBoost: {'colsample_bytree': 1, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Best Cross-Validation F1 Macro Score for XGBoost: 0.9677326905863056\n",
      "\n",
      "Performing K-Fold cross-validation for Stacking Model...\n",
      "Stacking Model Cross-Validation F1 Macro Scores: [0.97757153 0.96920463 0.96869913 0.96960537 0.97766717]\n",
      "Average F1 Macro Score for Stacking Model: 0.9725495679460924\n",
      "\n",
      "Test Set Evaluation:\n",
      "Test Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Cycling       0.93      0.93      0.93        55\n",
      "         Football       0.73      0.78      0.75        78\n",
      "          Jogging       1.00      1.00      1.00         2\n",
      "         JumpRope       0.94      0.65      0.77        23\n",
      "          Pushups       1.00      0.67      0.80         3\n",
      "          Sitting       0.99      0.98      0.99       139\n",
      "         Swimming       0.86      0.72      0.78        25\n",
      "           Tennis       0.68      0.82      0.74       116\n",
      "          Walking       0.79      0.92      0.85        12\n",
      "WalkingDownstairs       0.67      0.35      0.46        17\n",
      "  WalkingUpstairs       0.56      0.26      0.36        19\n",
      "\n",
      "         accuracy                           0.82       489\n",
      "        macro avg       0.83      0.73      0.77       489\n",
      "     weighted avg       0.83      0.82      0.82       489\n",
      "\n",
      "Confusion Matrix (Test):\n",
      " [[ 51   0   0   0   0   0   0   2   0   1   1]\n",
      " [  0  61   0   1   0   1   0  15   0   0   0]\n",
      " [  0   0   2   0   0   0   0   0   0   0   0]\n",
      " [  0   2   0  15   0   0   0   6   0   0   0]\n",
      " [  1   0   0   0   2   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0 136   0   1   0   1   0]\n",
      " [  0   0   0   0   0   0  18   7   0   0   0]\n",
      " [  1  16   0   0   0   0   2  95   1   1   0]\n",
      " [  0   0   0   0   0   0   1   0  11   0   0]\n",
      " [  1   1   0   0   0   0   0   6   0   6   3]\n",
      " [  0   4   0   0   0   0   0   8   2   0   5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.combine import SMOTEENN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Loading dataset\n",
    "data = pd.read_csv('har_train.csv')\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Label Encoding for y\n",
    "labelen = LabelEncoder()\n",
    "yencoded = labelen.fit_transform(y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, yencoded, test_size=0.2, stratify=yencoded, random_state=seed)\n",
    "\n",
    "# Applying SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "st = SMOTETomek(random_state=seed)\n",
    "Xresamp, yresamp = st.fit_resample(Xtrain, ytrain)\n",
    "\n",
    "from collections import Counter\n",
    "print(f\"New class distribution: {Counter(yresamp)}\")\n",
    "\n",
    "# Class weights for Random Forest\n",
    "rfweights = {i: 1/count for i, count in Counter(yresamp).items()}\n",
    "\n",
    "# Class weights for XGBoost\n",
    "cw = compute_class_weight('balanced', classes=np.unique(yresamp), y=yresamp)\n",
    "cwdict = dict(zip(np.unique(yresamp), cw))\n",
    "sw = np.array([cwdict[label] for label in yresamp])\n",
    "\n",
    "# Parameter grid for RF\n",
    "rf = RandomForestClassifier(class_weight=rfweights, random_state=seed)\n",
    "rfparameters = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "\"\"\"  These are the best training parameter grid used\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "    'max_features': ['sqrt']\"\"\"\n",
    "\n",
    "# Parameter grid for XGB (Best parameters)\n",
    "xgb = XGBClassifier(random_state=seed, eval_metric='mlogloss')\n",
    "xgbparameters = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.8, 1],\n",
    "    'colsample_bytree': [0.8, 1]\n",
    "}\n",
    "\n",
    "\"\"\" These are the best param grid for XGB\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [10],\n",
    "    'learning_rate': [0.1],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [1]\"\"\"\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "nsplits = 5\n",
    "kf = StratifiedKFold(n_splits=nsplits, shuffle=True, random_state=seed)\n",
    "\n",
    "# Cross-validation for Random Forest\n",
    "print(\"\\nPerforming K-Fold cross-validation for Random Forest...\")\n",
    "rfgrid = GridSearchCV(rf, rfparameters, scoring='f1_macro', cv=kf, n_jobs=-1)\n",
    "rfgrid.fit(Xresamp, yresamp)\n",
    "rfbest = rfgrid.best_estimator_\n",
    "\n",
    "# Best parameters and cross-validation score for Random Forest\n",
    "print(\"Best Parameters for Random Forest:\", rfgrid.best_params_)\n",
    "print(\"Best Cross-Validation F1 Macro Score for Random Forest:\", rfgrid.best_score_)\n",
    "\n",
    "# Cross-validation for XGBoost\n",
    "print(\"\\nPerforming K-Fold cross-validation for XGBoost...\")\n",
    "xgbgrid = GridSearchCV(xgb, xgbparameters, scoring='f1_macro', cv=kf, n_jobs=-1)\n",
    "xgbgrid.fit(Xresamp, yresamp, sample_weight=sw)\n",
    "xgbbest = xgbgrid.best_estimator_\n",
    "\n",
    "# Best parameters and cross-validation score for XGBoost\n",
    "print(\"Best Parameters for XGBoost:\", xgbgrid.best_params_)\n",
    "print(\"Best Cross-Validation F1 Macro Score for XGBoost:\", xgbgrid.best_score_)\n",
    "\n",
    "# Stacking Model with RF and XGB\n",
    "estimators = [\n",
    "    ('rf', rfbest),\n",
    "    ('xgb', xgbbest)\n",
    "]\n",
    "mmodel = LogisticRegression(random_state=seed)\n",
    "\n",
    "finalclf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=mmodel,\n",
    "    cv=kf,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "print(\"\\nPerforming K-Fold cross-validation for Stacking Model...\")\n",
    "scores = cross_val_score(finalclf, Xresamp, yresamp, cv=kf, scoring='f1_macro', n_jobs=-1)\n",
    "print(\"Stacking Model Cross-Validation F1 Macro Scores:\", scores)\n",
    "print(\"Average F1 Macro Score for Stacking Model:\", np.mean(scores))\n",
    "\n",
    "finalclf.fit(Xresamp, yresamp)\n",
    "\n",
    "#Validation test\n",
    "ypred = finalclf.predict(Xtest)\n",
    "\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "print(\"Test Report:\\n\", classification_report(ytest, ypred, target_names=labelen.classes_))\n",
    "print(\"Confusion Matrix (Test):\\n\", confusion_matrix(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a78521b-234d-41c1-99d7-e657838e88c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T2.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Saving the trained model\n",
    "joblib.dump(finalclf, 'T2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97ee7e-dc88-4cbe-a810-89f490551a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
